"use strict";(globalThis.webpackChunkdocumentation=globalThis.webpackChunkdocumentation||[]).push([[7289],{28453(e,t,n){n.d(t,{R:()=>i,x:()=>r});var a=n(96540);const s={},o=a.createContext(s);function i(e){const t=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:t},e.children)}},96534(e,t,n){n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"server-admin/admin-tips/duckdb","title":"duckdb","description":"This content is based on a message from Roy Mendelssohn to the ERDDAP users group.","source":"@site/docs/server-admin/admin-tips/duckdb.md","sourceDirName":"server-admin/admin-tips","slug":"/server-admin/admin-tips/duckdb","permalink":"/pl/docs/server-admin/admin-tips/duckdb","draft":false,"unlisted":false,"editUrl":"https://github.com/erddap/erddap.github.io/tree/main/docs/server-admin/admin-tips/duckdb.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docSidebar","previous":{"title":"Admin Tips","permalink":"/pl/docs/category/admin-tips"},"next":{"title":"optimizing_netcdf","permalink":"/pl/docs/server-admin/admin-tips/optimizing_netcdf"}}');var s=n(74848),o=n(28453);const i={},r=void 0,d={},c=[];function l(e){const t={a:"a",code:"code",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["This content is based on a ",(0,s.jsx)(t.a,{href:"https://groups.google.com/g/erddap/c/6Hl024ZGkes/m/DS5WzsydAQAJ",children:"message from Roy Mendelssohn to the ERDDAP users group"}),"."]}),"\n",(0,s.jsx)(t.p,{children:"ERDDAP\u2122 tries to be agnostic about what data formats people use for their data, instead trying to work with the data formats of most use to the communities we mainly serve. As more and more work is in the cloud, and there are a plethora of data formats that people use in the cloud, it would be nice if ERDDAP\u2122 could support a lot of these formats. Alas, ERDDAP\u2122 development and maintenance is already understaffed, and what would be desirable is to make use of the work of others to achieve this goal, without having to modify ERDDAP\u2122."}),"\n",(0,s.jsxs)(t.p,{children:["Enter ",(0,s.jsx)(t.a,{href:"https://duckdb.org/",children:"DuckDB"})," and ",(0,s.jsx)(t.a,{href:"https://trino.io/",children:"Trino"}),". Both of these provide connections to a variety of data formats, and can be accessed using JDBC. In this writeup I will only look at using DuckDB because I have some familiarity with it, and Trino at least to my mind, seemed a little more complicated to set up, and right now I just wanted a proof of concept. (A while back Damien Smythe had written that he had sussed out how to use Trino and I contacted him and he did get it to work and will send me notes - so more on this in the future). Also I am anything but an expert on any of this, so this is learning along with me - let me know if you find any mistakes, or things that are unclear, or if there are better ways of doing things."]}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Preliminaries:\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"In order to use DuckDB in ERDDAP\u2122 you need to install duckDB onto your computer, and to install the DuckDB JDBC connector into ERDDAP\u2122. Instructions on installing DuckDB itself are at"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://duckdb.org/docs/installation/?version=stable&environment=cli&platform=macos&download_method=direct",children:"https://duckdb.org/docs/installation/?version=stable&environment=cli&platform=macos&download_method=direct"})}),"\n",(0,s.jsx)(t.p,{children:"The DuckDB JDBC connector can be downloaded from"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://duckdb.org/docs/stable/clients/java.html",children:"https://duckdb.org/docs/stable/clients/java.html"})}),"\n",(0,s.jsx)(t.p,{children:"If your tomcat is located at $TOMCAT_HOME, then you want to put this file at $TOMCAT_HOME/webapps/erddap/WEB-INF/lib."}),"\n",(0,s.jsxs)(t.ol,{start:"2",children:["\n",(0,s.jsx)(t.li,{children:"What makes this tricky:\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"At least for DuckDB (as I have said I haven\u2019t looked at Trino in any detail) Java programs can only access DuckDB through the JDBC connector, and the JDBC connector can only read data that are stored in the DuckDB .db format. While DuckDB is a great tool, having to copy the data to this format defeats the whole purpose, However, DuckDB provides the facility that for many of the file formats it supports (there is a growing list of extensions) you can set up a virtual .db file that just reads in the necessary metadata. I have found the .db that is created to be quite small in size, and provides what appears to be fast access. An interesting note is that unlike most database systems (and including Trino), with DuckDB you do not have an application that is constantly running in the background, so there must be some lag in access due to startup, but as I said in my small tests access seems speedy."}),"\n",(0,s.jsxs)(t.ol,{start:"3",children:["\n",(0,s.jsx)(t.li,{children:"A simple example:\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"To start, I will look at how to do this with a .csv file. Correct, this isn\u2019t that interesting of a use case as ERDDAP\u2122 already handles .csv files pretty well, so nothing is being gained in that sense, but a text file is easier to work with and to debug in seeing what is going on, and as I was learning this it made my life easier. Below I will look at a more interesting use case once we see what is needed."}),"\n",(0,s.jsx)(t.p,{children:"The file I am working with is called \u201cdetects.csv\u201d which is a 305MB file of tag detections by sensors. The first step is to create a .db file that points to this file (in this and what follows for convenience I will always assume you are working at the level needed so I do not give full paths - in practice you will have to):"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"duckdb detects.db <<EOF\nCREATE VIEW detects_table AS\nSELECT *\nFROM read_csv_auto(\n'detects.csv',\nheader=true,\nsample_size=2147483647 -- scan whole file for schema (optional)\n);\nEOF\n"})}),"\n",(0,s.jsx)(t.p,{children:"The file \u201cdetects.db\u201d is 268KB in size. We can check that this has actually worked:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:'duckdb detects.db\nDuckDB v1.3.1 (Ossivalis) 2063dda3e6\nEnter ".help" for usage hints.\nD SHOW TABLES;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name \u2502\n\u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 detects_table \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nD SELECT * FROM detects_table LIMIT 5;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Study_ID \u2502 TagCode \u2502 DateTime_PST \u2502 recv \u2502 location \u2502 \u2026 \u2502 tag_life \u2502 Rel_latitude \u2502 Rel_longitude \u2502 time \u2502\n\u2502 varchar \u2502 varchar \u2502 timestamp \u2502 int64 \u2502 varchar \u2502 \u2502 int64 \u2502 double \u2502 double \u2502 varchar \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 ButteSink_2024 \u2502 1B76 \u2502 NULL \u2502 NULL \u2502 NULL \u2502 \u2026 \u2502 52 \u2502 39.32663 \u2502 -121.8847 \u2502 -08 \u2502\n\u2502 ButteSink_2024 \u2502 1B94 \u2502 NULL \u2502 NULL \u2502 NULL \u2502 \u2026 \u2502 52 \u2502 39.32663 \u2502 -121.8847 \u2502 -08 \u2502\n\u2502 ButteSink_2024 \u2502 1BA5 \u2502 NULL \u2502 NULL \u2502 NULL \u2502 \u2026 \u2502 52 \u2502 39.35858 \u2502 -121.8943 \u2502 -08 \u2502\n\u2502 ButteSink_2024 \u2502 1C55 \u2502 NULL \u2502 NULL \u2502 NULL \u2502 \u2026 \u2502 52 \u2502 39.33546 \u2502 -121.8923 \u2502 -08 \u2502\n\u2502 ButteSink_2024 \u2502 1C95 \u2502 NULL \u2502 NULL \u2502 NULL \u2502 \u2026 \u2502 52 \u2502 39.33546 \u2502 -121.8923 \u2502 -08 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 5 rows 18 columns (9 shown) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,s.jsx)(t.p,{children:"and the size of detects.db has not changed from doing this operation. So the final step is to connect this with ERDDAP\u2122, I show only the part needed to connect to the file, not all the variable information:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:'<dataset type="EDDTableFromDatabase" datasetID="duckdb_test" active="true">\n\x3c!-- JDBC Connection Details --\x3e\n<sourceUrl>jdbc:duckdb:detects.db</sourceUrl>\n<driverName>org.duckdb.DuckDBDriver</driverName>\n<catalogName></catalogName>\n<schemaName></schemaName>\n<tableName>detects_table</tableName>\n<columnNameQuotes></columnNameQuotes>\n>\n'})}),"\n",(0,s.jsx)(t.p,{children:"And this is the dataset in an ERDDAP\u2122 on my laptop using the DuckDB connector:"})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);