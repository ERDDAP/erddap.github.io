Цей вміст ґрунтується на [повідомлення від Roy Mendelssohn до ERDDAP група користувачів](https://groups.google.com/g/erddap/c/JWoS_y3cygg/m/zCpcNTxNAAAJ) й

1. Оптимізація файлів Netcdf для хмари
————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

JavaScript licenses API Веб-сайт

Останнім часом, коли я побачила цю дуже цікава стаття:

https://nsidc.github.io/cloud-optimized-icesat2/

Не дивлячись на те, що мова йде про мов програмування, редактори та формати файлів, і це не рекомендація якого формату (й) ви повинні використовувати, але досить зрозуміти, що в цьому папері і побачити, скільки поліпшення можна отримати ( ERDDAP™ завжди намагався бути діагностикою про багато цих питань, досить вибір спробувати і працювати з тим, як люди насправді працюють з даними) й

Папір в основному спрямований на ситуації, де дані зберігаються в магазині об'єктів, таких як Amazon S3. Об'ємні магазини з'єднуються з мережею http  (й) команди, так що у порівнянні з зберіганням з прямим підключенням до (Віртуальний) сервер, є набагато більш тривалий термін дії, оскільки запит повинен зробити круглу подорож. Для об'єктів, які ви хочете зробити якомога менше запитів, але якщо ви просто зробите дійсно великі запити, щоб зменшити кількість дзвінків, ви можете отримати доступ до більшої кількості даних, ніж вам потрібно, які можуть бути однаково повільними, якщо не більше так. Отже, для досягнення балансу між цими двома факторами. І хоча доступ до даних на об'єктах значно покращився, тому має доступ до безпосередньо прикріпленого сховища. У дослідженні це деякі оцінки:

Місцевий диск:
• час: 0.1мс
• 6 шукає: 0,6м (недбалий) 
• Читання метаданих скатертих швидко
Хмарний HTTP:
• 100-200м
• 6 запитів: 600-1200м (дуже повільно&#33;) 
• Кожен запит має мережевий цілодобовий час

Друге, щоб зрозуміти, що файли netcdf4/hdf5 зберігаються в шматках і повернуті на сторінках, тому відносний розмір кожного з цих може дійсно вплинути на швидкість доступу при доступі з магазину об'єкта, і це за замовчуванням метадані про файл розкидані по всьому файлу, тому отримання метаданих може знадобитися кілька запитів. Основна точка паперу полягає в тому, що розмір сторінки за замовчуванням для netcdf4/hdf5 файлів 4096 bytes (4КБ) до (що страшно для хмари&#33;) Оскільки розмір метаданих один, швидше за все, більше, ніж це, швидше за все, ваші розміри шматків також більше, ніж це. Отже, екстракт зажадає багато круглих планів, які повільно. Що ви хочете зробити це перепакувати файл таким чином, щоб всі метадані були на “верхній” файлу, і розмір сторінки не менше, як великий розмір метаданих плюс розмір одного шматка. Також за замовчуванням розмір сторінки не фіксується, але використовує стратегію, яка змінюється. Який папір, знайдений за допомогою фіксованого розміру сторінок, що виробляється кращими результатами.

Як визначити розмір метаданих файлів?

> h5stat yourfile.nc | grep "File metadata" # metadata size
>

І як я можу визначити розмір шматка:

> h5dump -pH MUR41_file.nc | grep -A3 CHUNKED
>

або

> ncdump -sh MUR41_file.nc | grep ChunkSizes
>

І як я можу визначити стратегію розвитку сторінки:

> h5stat yourfile.nc | grep "File space management strategy"
>

Ми можемо самі зателефонувати одержувачу і узгодити зручний час і місце вручення квітів, а якщо необхідно, то збережемо сюрприз.

Як я можу перепакувати файл netcdf, щоб всі метадані були на передній, і змінити стратегію, так що використовується розмір фіксованої сторінки, і розмір сторінки для використання? Правила великого пальця, який я знайшов:

Вибір розміру сторінки:
• Необхідно ≥ загальний розмір метаданих файлів (критичний&#33;) 
• Чи повинен бути живлення 2 (4МБ, 8МБ, 16МБ та ін.) 
• Не йти божевільний великий - 32MB, як правило, практичний макс
• Розглянемо розміри горбів - розмір сторінки повинен вмістити найбільші шматки

Як зазначено вище, в ідеалі розмір повинен бути більшим, ніж розмір метаданих плюс розмір одного шматка. Що було встановлено дослідження, що для багатьох даних розмір сторінки 8MB є хорошим торговим пунктом, це, ймовірно, більше, ніж розмір метаданих + розмір шматка шматка, і не означає більше даних, ніж вам потрібно. Для цього:

h5repack -S PAGE -G 8388608 Ваш файл .nc JavaScript licenses API Веб-сайт .nc 

Тут є значення для отримання різних розмірів сторінок:

4194304 (4МБ) 
8388608 (8МБ) 
16777216 (16МБ) 
33554432 (32МБ) 

б. Чи є переваги, якщо використовувати файли локально, а також?

Папір та інші речі, які я знайшов, свідчать про те, що навіть локально можна отримати швидкість в будь-який час від 10%-30%. На мій що-небудь, але вичерпні тести я знайшов приріст швидкості близько 10%, коли запити відносно невеликі порівняно з загальним розміром файлів, і збільшення швидкості, як результат отримує більший, але я ніколи не знайшов його повільніше.

з. АТАСТАФЛ

Ах, але є багато чого ловити кудись, це, як безкоштовний обід. І ловець полягає в тому, що розмір фіксованої сторінки збільшується розмір файлу. Для деяких випадків я спробував:

617М р1 .nc 
632M mur1_оптимізований .nc 
608М р2 .nc 
616M mur2_оптимізований .nc 
29М хла1 .nc 
40M chla1_оптимізований .nc 
30М хла2 .nc 
40M chla2_оптимізований .nc 

Таким чином, торговий пункт є не незначне збільшення розміру файлу.

до. Але якщо я повинен переробити файли будь-яким чином...?

Хороше питання, якщо я повинен писати скрипт для обробки файлів, чому не просто писати скрипт для перекладу в формат, як кажуть zarr? zarr має багато пропонентів і якщо ви зацікавлені в zarr просто зробити швидкий пошук смоктатиduckgo і є багато хороших постів, можливо, більш збалансований вигляд є наhttps://www.youtube.com/watch?v=IEAcCmcOdJs  (Цікаво, що багато точок, які він піднімає, що формат льодяника намагається звернутися) й Отже, чому ви не хочете перевести ваші файли на щось, як zarr, Спочатку, якщо ви створюєте файли netcdf, ви можете почати оптимізацію файлів з цього часу, який з часом буде бачити швидкість наростає і ви не повинні переформатувати останні файли, і ви не повинні переформатувати останні файли, і ви не повинні переформатувати останні файли, і ви не повинні переформатувати останні файли, і ви не повинні переформатувати останні файли, і ви не повинні переформатувати останні файли, і ERDDAP™ все ще вдасться агрегатувати файли, хоча деякі внутрішні налаштування відрізняються. По-друге, ви можете мати багато інструментів, які залежать від файлів netcdf, і цей підхід буде означати, що не маючи реінструменту, який може бути великим обсягом коду. Вкажіть, що найкраще працює для вашої ситуації. Як нагадувати, якщо ви вирішили використовувати zarr файли з ERDDAP™ , вони повинні бути zarr формат v2 файлів.

е. Великі дані - aside

Великі дані говорять про багато, але як велика кількість даних, які використовують більшість людей і як це порівняти з можливостями сучасних ноутбуків (так ноутбуки, не сервери) й Цікаво:

https://www.youtube.com/watch?v=GELhdezYmP0Почати хвилину 37, хоча всі розмови цікаві

У дослідженні він згадує:

https://motherduck.com/blog/redshift-files-hunt-for-big-data/

Так є порівняно невеликий відсоток користувачів, які дійсно потрібно відвертати потужність, але переважна більшість користувачів може зробити їх аналіз на ноутбуці, зовнішні диски 26TB тепер за 300 доларів і чутки, що зовнішні диски 60TB будуть доступні до кінця року. Щось подумати.

2. Використання ERDDAP™ з Google Cloud Platform або іншими хмарними провайдерами, крім AWS
----------------

На даний момент ERDDAP™ Відомий тільки для роботи з магазинами об'єктів AWS (С3) , хоча поліпшення і узагальнення ERDDAP™ Підтримка об'єкта - на список (Переглянутиhttps://github.com/ERDDAP/erddap/issues/158) й Що робити, якщо ви сказали, що ви повинні запустити ERDDAP™ на Google Cloud Platform (ГП) чи схожа платформа? В першу чергу більшість хмарних платформ пропонують різні рівні зберігання, як правило, в тому числі один, який схожий на локальне зберігання і визнається операційною системою, яка підключена до мережі, зазвичай, використовує NFS для доступу (знову доступна ОС) , і один, який є магазином об'єктів. Перше рішення не використовувати об'єкти, і ви будете добре йти. Але як завжди, TANSTAAFL і недолік в цьому випадку, як ви йдете з магазину об'єктів -&gt; NFS access -&gt; локальний магазин ваших витрат також підійшов. (Я б додав, що NFS також отримує доступ до мережі, а також має власні проблеми, які відповідають вимогам файлів.) й

Якщо ви повинні використовувати магазин об'єктів, або можете дозволити тільки магазин об'єкта, відповідь - файлова система FUSE (https://github.com/libfuse/libfuse) й На GCP це називається gcsfuse, і кроки для його установки:

• Встановлення gcsfuse на вашому GCP Linux зображення:
sudo apt оновлення
sudo apt встановити gcsfuse
• Ауттентифікат до GCP (якщо вже не автентифіковано) :
Переконайтеся, що у вас є право, як правило, через обліковий запис служби або за допомогою запуску gcloud auth логін.
• Встановити відро GCS до місцевого каталогу:
Встановити відро GCS до місцевого каталогу за допомогою gcsfuse. Цей приклад дозволяє отримати доступ до даних, якщо він був частиною локальної файлової системи.
JavaScript licenses API Веб-сайт Go1.13.8

І тепер ваш магазин об'єктів можна отримати, як це частина файлової системи Linux, тому буде працювати з ERDDAP™ й Це, здається, магія, отримання найкращого з обох світів, має бути вловим. І є. Файлові системи FUSE є непоганим, ніж доступ до магазину об'єкта безпосередньо (В основному ви додали ще один шар доступу) й У моїй дослідницькій кошторисці, як набагато повільніше, над картою, тому я не знаю, скільки повільніше. Але якщо ви перебуваєте в ситуації, де ви повинні запустити на GCP за допомогою об'єктів, у вас є рішення для того, що буде працювати з ERDDAP™ й

3. Що ви можете зробити зараз, щоб допомогти.
————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————

Якщо у вас є час і можливість перевірити деякі з цих речей і звітувати за результатами, які будуть чудовими. Тим не менш, якщо у вас є доступ до GCP або аналогічний і подивитися, скільки повільніше ERDDAP™ доступ за допомогою FUSE (також ви можете перевірити це на AWS також) й Якщо швидкість штрафу не дуже великий, що буде чудовим, тому що я маю на увазі, що деякі люди будуть скоро повинні працювати їх ERDDAP™ s на GCP з магазином об'єктів. так це не просто справа теоретичного інтересу.
