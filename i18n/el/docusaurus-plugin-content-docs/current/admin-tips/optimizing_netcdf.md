Το περιεχόμενο αυτό βασίζεται σε [Μήνυμα από Roy Mendelssohn στο ERDDAP Ομάδα χρηστών](https://groups.google.com/g/erddap/c/JWoS_y3cygg/m/zCpcNTxNAAAJ) .

1. Βελτιστοποίηση αρχείων netcdf για το σύννεφο
—————————————————————

α. επανασυσκευασία και μέγεθος σελίδας

Πρόσφατα στην έρευνα βρήκα αυτό το πολύ ενδιαφέρον άρθρο:

https://nsidc.github.io/cloud-optimized-icesat2/

Τίποτα δεν φαίνεται να προκαλεί πάθη όπως συζητήσεις γλωσσών προγραμματισμού, συντάκτες, και μορφές αρχείων, και αυτό δεν είναι μια σύσταση του τι μορφή (α) θα πρέπει να χρησιμοποιήσετε, αλλά μάλλον για να καταλάβετε τι υπάρχει σε αυτό το χαρτί και να δείτε πόση βελτίωση μπορεί να επιτευχθεί ( ERDDAP™ πάντα προσπαθούσε να είναι αγνωστικιστής για πολλά από αυτά τα θέματα, προτιμώντας να προσπαθήσει και να συνεργαστεί με το πώς οι άνθρωποι εργάζονται πραγματικά με τα δεδομένα) .

Το χαρτί απευθύνεται κυρίως σε καταστάσεις όπου τα δεδομένα αποθηκεύονται σε ένα κατάστημα αντικειμένων όπως το Amazon S3. Τα καταστήματα αντικειμένων έχουν πρόσβαση μέσω του δικτύου χρησιμοποιώντας http  (α) εντολές, έτσι σε σύγκριση με την αποθήκευση με άμεση σύνδεση με το (εικονική) server, υπάρχει πολύ μεγαλύτερη καθυστέρηση καθώς η αίτηση πρέπει να κάνει ένα ταξίδι μετ' επιστροφής. Για τα καταστήματα αντικειμένων που θέλετε να κάνετε όσο το δυνατόν λιγότερα αιτήματα, αλλά αν απλά κάνετε πραγματικά μεγάλα αιτήματα για τη μείωση του αριθμού των κλήσεων, μπορεί να έχετε πρόσβαση σε πολύ περισσότερα δεδομένα από ό, τι χρειάζεστε, τα οποία μπορεί να είναι εξίσου αργά αν όχι περισσότερο. Έτσι το κόλπο είναι να επιτευχθεί μια ισορροπία μεταξύ αυτών των δύο παραγόντων. Και παρόλο που η πρόσβαση σε δεδομένα σε καταστήματα αντικειμένων έχει βελτιωθεί σημαντικά, έτσι έχει πρόσβαση σε απευθείας προσαρτημένη αποθήκευση. Κατά την έρευνα αυτού ορισμένες εκτιμήσεις είναι:

Τοπικός δίσκος:
• Χρόνος αναζήτησης: 0.1ms
• 6 επιδιώξεις: 0.6ms (αμελητέα) 
• Η ανάγνωση διάσπαρτων μεταδεδομένων είναι γρήγορη
Σύννεφο HTTP:
• Απαίτηση καθυστέρησης: 100-200ms
• 6 αιτήσεις: 600-1200ms (Πολύ αργά&#33;) 
• Κάθε αίτημα έχει χρόνο διαδρομής δικτύου

Το δεύτερο πράγμα που πρέπει να καταλάβετε είναι ότι τα αρχεία netcdf4/hdf5 αποθηκεύονται σε κομμάτια και επιστρέφονται σε σελίδες, έτσι το σχετικό μέγεθος καθενός από αυτούς μπορεί πραγματικά να επηρεάσει την ταχύτητα πρόσβασης όταν η πρόσβαση είναι από ένα κατάστημα αντικειμένων, και ότι εξ ορισμού τα μεταδεδομένα σχετικά με το αρχείο είναι διάσπαρτα σε όλο το αρχείο, οπότε η λήψη των μεταδεδομένων μπορεί να λάβει αρκετά αιτήματα. Το κύριο σημείο του χαρτιού είναι ότι το προκαθορισμένο μέγεθος σελίδας για τα αρχεία netcdf4/hdf5 είναι 4096 bytes (4KB) - Ναι. (που είναι τρομερό για το σύννεφο&#33;) Δεδομένου ότι το μέγεθος μεταδεδομένων και μόνο είναι πιθανώς μεγαλύτερο από αυτό και περισσότερο από πιθανό μέγεθος κομματιών σας είναι επίσης μεγαλύτερο από αυτό. Έτσι, ένα εκχύλισμα θα απαιτήσει πολλά στρογγυλά ταξίδια που είναι αργή. Αυτό που θέλετε να κάνετε είναι να επανασυσκευάσετε το αρχείο έτσι ώστε όλα τα μεταδεδομένα να βρίσκονται στην “κορυφή” του αρχείου, και ότι το μέγεθος της σελίδας είναι τουλάχιστον τόσο μεγάλο όσο το μέγεθος των μεταδεδομένων συν το μέγεθος ενός κομματιού. Επίσης από προεπιλογή το μέγεθος της σελίδας δεν είναι σταθερό, αλλά χρησιμοποιεί μια στρατηγική που ποικίλλει. Αυτό που βρέθηκε στο χαρτί είναι η χρήση ενός σταθερού μεγέθους σελίδας που παρήγαγε καλύτερα αποτελέσματα.

Πώς μπορώ να προσδιορίσω το μέγεθος μεταδεδομένων αρχείων;

> h5stat yourfile.nc | grep "File metadata" # metadata size
>

Και πώς μπορώ να καθορίσω το μέγεθος του κομματιού:

> h5dump -pH MUR41_file.nc | grep -A3 CHUNKED
>

ή

> ncdump -sh MUR41_file.nc | grep ChunkSizes
>

Και πώς μπορώ να καθορίσω τη στρατηγική μεγέθους σελίδας:

> h5stat yourfile.nc | grep "File space management strategy"
>

Το πιο πιθανό αυτή η εντολή θα επιστρέψει “H5F_FSPACE_STRATEGY_FSM_AGGR” που είναι η προκαθορισμένη στρατηγική και αυτό που θέλουμε να επιστρέψει είναι “H5F_FSPACE_STRATEGY_PAGE”

Πώς μπορώ να ξανασυσκευάσω το netcdf αρχείο μου έτσι ώστε όλα τα μεταδεδομένα να βρίσκονται στο μπροστινό μέρος, και να αλλάξω τη στρατηγική ώστε να χρησιμοποιηθεί ένα σταθερό μέγεθος σελίδας, και ποιο μέγεθος σελίδας να χρησιμοποιηθεί; Οι κανόνες του αντίχειρα που βρήκα είναι:

Επιλογή μεγέθους σελίδας:
• Πρέπει να είναι ≥ συνολικό μέγεθος μεταδεδομένων αρχείων (Κρίσιμη&#33;) 
• Θα πρέπει να είναι ισχύς 2 (4MB, 8MB, 16MB, κ.λπ.) 
• Μην τρελαίνεσαι μεγάλο - 32MB είναι συνήθως το πρακτικό max
• Εξετάστε τα μεγέθη κομματιών - μέγεθος σελίδας θα πρέπει να φιλοξενήσει τα μεγαλύτερα κομμάτια

Όπως προαναφέρθηκε, ιδανικά το μέγεθος πρέπει να είναι μεγαλύτερο από το μέγεθος μεταδεδομένων συν το μέγεθος ενός κομματιού. Αυτό που βρήκε η μελέτη είναι ότι για πολλά σύνολα δεδομένων το 8MB μέγεθος σελίδας είναι μια καλή ανταλλαγή, είναι πιθανώς μεγαλύτερο από το μέγεθος μεταδεδομένων + μέγεθος κομματιών, και δεν τραβάει πολύ περισσότερα δεδομένα από ό, τι χρειάζεστε. Για να επιτευχθεί αυτό:

h5repack - ΣΕΛΙΔΑ -G 8388608 yourfile .nc το αρχείο σας_ βελτιώθηκε .nc 

Εδώ είναι οι τιμές που θα χρησιμοποιηθούν για να πάρει διαφορετικά μεγέθη σελίδων:

4194304 (4MB) 
8388608 (8MB) 
16777216 (16MB) 
33554432 (32MB) 

β. Υπάρχουν οφέλη αν χρησιμοποιείτε αρχεία και τοπικά;

Το χαρτί και άλλα πράγματα που βρήκα δείχνουν ότι ακόμα και τοπικά μπορεί να υπάρξει κέρδος ταχύτητας οπουδήποτε από 10%-30%. Σε οτιδήποτε, εκτός από εξαντλητικές δοκιμές βρήκα κέρδη ταχύτητας περίπου 10% όταν τα αιτήματα είναι σχετικά μικρά σε σύγκριση με το συνολικό μέγεθος αρχείου, και η αύξηση της ταχύτητας μειώνεται καθώς το αίτημα μεγαλώνει, αλλά ποτέ δεν το βρήκα να είναι πιο αργό.

γ. ΤΑΝΤΑΑΦΛ

Αχ, αλλά υπάρχουν πολλά για να πιάσεις κάπου, αυτό φαίνεται σαν δωρεάν γεύμα. Και η ψαριά είναι ότι το σταθερό μέγεθος σελίδας αυξάνει το μέγεθος του αρχείου. Για μερικές από τις υποθέσεις που προσπάθησα:

617M mur1 .nc 
632M mur1_βελτιωμένη .nc 
608M mur2 .nc 
616M mur2_βελτιωμένη .nc 
29M chla1 .nc 
40M chla1_βελτιωμένη .nc 
30M chla2 .nc 
40M chla2_βελτιωμένη .nc 

Έτσι, η ανταλλαγή είναι ότι υπάρχει μια όχι ασήμαντη αύξηση στο μέγεθος του αρχείου.

δ. Αλλά αν χρειαστεί να επεξεργαστώ ξανά τα αρχεία...

Μια καλή ερώτηση είναι αν πρέπει να γράψω ένα σενάριο για να επεξεργαστώ ξανά τα αρχεία, γιατί να μην γράψω απλά ένα σενάριο για να μεταφράσω σε μια μορφή όπως να πω zarr? zarr έχει πολλούς υποστηρικτές και αν ενδιαφέρεστε για zarr απλά να κάνετε μια γρήγορη αναζήτηση duckduckgo και εκεί πολλές καλές θέσεις, μια ίσως πιο ισορροπημένη άποψη είναι στοhttps://www.youtube.com/watch?v=IEAcCmcOdJs  (Είναι ενδιαφέρον ότι πολλά από τα σημεία που εγείρει είναι αυτό που η μορφή παγάκια προσπαθούν να αντιμετωπίσουν) . Γιατί λοιπόν μπορεί να μην θέλετε να μεταφράσετε τα αρχεία σας σε κάτι σαν zarr, Πρώτον, αν δημιουργήσετε αρχεία netcdf τακτικά, θα μπορούσατε να ξεκινήσετε τη βελτιστοποίηση των αρχείων από τώρα και στο εξής, η οποία με την πάροδο του χρόνου θα δείτε κέρδη ταχύτητας και δεν θα χρειαστεί να αναμορφώσετε τα παρελθόντα αρχεία, και ERDDAP™ θα είναι ακόμα σε θέση να συγκεντρώσει πάνω από τα αρχεία παρόλο που ορισμένες από τις εσωτερικές ρυθμίσεις διαφέρουν. Δεύτερον, μπορεί να έχετε πολλά εργαλεία που εξαρτώνται από τα αρχεία netcdf, και αυτή η προσέγγιση θα σήμαινε ότι δεν χρειάζεται να επαναλειτουργήσει αυτό που θα μπορούσε να είναι μια εκτεταμένη ποσότητα κώδικα. Το θέμα είναι να γνωρίζετε τις επιλογές και να επιλέξετε τι λειτουργεί καλύτερα για την κατάστασή σας. Ακριβώς ως υπενθύμιση, αν επιλέξετε να χρησιμοποιήσετε zarr αρχεία με ERDDAP™ , πρέπει να είναι zarr μορφή v2 αρχεία.

Ε. Μεγάλα δεδομένα - μια άκρη

Μεγάλα δεδομένα μιλούν για πολλά, αλλά πόσο μεγάλα είναι τα δεδομένα που χρησιμοποιούν οι περισσότεροι άνθρωποι και πώς αυτό συγκρίνεται με τις δυνατότητες των σύγχρονων φορητών υπολογιστών (ναι φορητοί υπολογιστές, όχι servers) . Μια ενδιαφέρουσα λήψη είναι:

https://www.youtube.com/watch?v=GELhdezYmP0Ξεκινήστε γύρω στο 37ο λεπτό αν και η όλη ομιλία είναι ενδιαφέρουσα

Η μελέτη που αναφέρει είναι:

https://motherduck.com/blog/redshift-files-hunt-for-big-data/

Έτσι, υπάρχει ένα σχετικά μικρό ποσοστό των χρηστών που πραγματικά πρέπει να ενεργοποιήσετε τη δύναμη, αλλά η συντριπτική πλειοψηφία των χρηστών μπορεί να κάνει τις αναλύσεις τους σε ένα laptop, 26TB εξωτερικές drives είναι τώρα κάτω από $ 300 και φήμες είναι ότι 60TB εξωτερικές drives θα είναι διαθέσιμες μέχρι το τέλος του έτους. Κάτι να σκεφτώ.

2. Χρήση ERDDAP™ με πλατφόρμα Google Cloud ή άλλους παρόχους cloud εκτός AWS
---------------------------------------------------------------------------------------------------------------------------------------

Αυτή τη στιγμή ERDDAP™ είναι γνωστό ότι εργάζεται μόνο με AWS καταστήματα αντικειμένων (S3) , βελτίωση και γενίκευση ERDDAP™ Η υποστήριξη αποθήκευσης αντικειμένων είναι στη λίστα todo (Βλέπεhttps://github.com/ERDDAP/erddap/issues/158) . Οπότε τι να κάνεις αν σου πουν ότι πρέπει να τρέξεις ERDDAP™ στην πλατφόρμα Google Cloud (GCP) ή μια παρόμοια πλατφόρμα; Πρώτον, οι περισσότερες πλατφόρμες cloud προσφέρουν διαφορετικά επίπεδα αποθήκευσης, συνήθως συμπεριλαμβανομένου ενός που είναι παρόμοιο με την τοπική αποθήκευση και αναγνωρίζεται από το λειτουργικό σύστημα, ένα που είναι συνδεδεμένο μέσω του δικτύου που χρησιμοποιεί συνήθως NFS για πρόσβαση (ξανά απευθείας προσβάσιμη από το λειτουργικό σύστημα) , και ένα που είναι ένα κατάστημα αντικειμένων. Η πρώτη λύση είναι να μην χρησιμοποιήσετε τα καταστήματα αντικειμένων, και θα ήταν καλό να πάτε. Αλλά όπως πάντα, TANSTAAFL και το μειονέκτημα σε αυτή την περίπτωση είναι όπως πάτε από το κατάστημα αντικειμένων -&gt; NFS πρόσβαση -&gt; τοπικό κατάστημα κόστος σας επίσης ανεβαίνει. (Θα ήθελα να προσθέσω ότι η NFS είναι επίσης προσβάσιμη μέσω του δικτύου, και έχει τα δικά της ζητήματα καθυστέρησης, αυτό θα ωφεληθεί επίσης από τη βελτιστοποίηση αρχείων) .

Εάν πρέπει να χρησιμοποιήσετε το κατάστημα αντικειμένων, ή μπορεί να αντέξετε οικονομικά μόνο ένα κατάστημα αντικειμένων, η απάντηση είναι ένα σύστημα αρχείων FUSE (https://github.com/libfuse/libfuse) . Στο GCP, αυτό ονομάζεται gcsfuse, και τα βήματα για την εγκατάστασή του είναι:

• Εγκαταστήστε το gcsfuse στην εικόνα GCP Linux:
ενημέρωση sudo apt
sudo apt εγκατάσταση gcsfuse
• Ταυτοποίηση GCP (εάν δεν έχει ήδη πιστοποιηθεί) :
Βεβαιωθείτε ότι έχετε τα σωστά διαπιστευτήρια, συνήθως μέσω του λογαριασμού υπηρεσιών ή με την εκτέλεση gloud auth σύνδεση.
• Προσάρτηση του κουβά GCS σε έναν τοπικό κατάλογο:
Προσάρτηση κουβά σας GCS σε έναν τοπικό κατάλογο χρησιμοποιώντας gcsfuse. Αυτό επιτρέπει στην GCP περίπτωση σας να έχει πρόσβαση στα δεδομένα σαν να ήταν μέρος του τοπικού συστήματος αρχείων.
gcsfuse το όνομά σας/path/to/mount/directory

Και τώρα το κατάστημα αντικειμένων σας μπορεί να προσπελαστεί σαν να είναι μέρος του συστήματος αρχείων Linux, έτσι θα λειτουργήσει με ERDDAP™ . Αυτό φαίνεται σαν μαγεία, να πάρει το καλύτερο και από τους δύο κόσμους, πρέπει να υπάρχει μια παγίδα. Και υπάρχει. Τα συστήματα αρχείων FUSE είναι λίγο πιο αργά από την πρόσβαση στο κατάστημα αντικειμένων άμεσα (βασικά έχετε προσθέσει ένα άλλο στρώμα στην πρόσβαση) . Στην έρευνά μου εκτιμώ πόσο πιο αργά είναι σε όλο τον χάρτη, οπότε δεν έχω ιδέα πόσο πιο αργά. Αλλά αν είστε σε μια κατάσταση όπου θα πρέπει να τρέξει σε GCP χρησιμοποιώντας καταστήματα αντικειμένων, έχετε μια λύση για τώρα που θα λειτουργήσει με ERDDAP™ .

3. Τι μπορείς να κάνεις τώρα για να βοηθήσεις.
—————————————

Αν έχετε το χρόνο και την ικανότητα να δοκιμάσετε μερικά από αυτά τα πράγματα και να αναφέρετε τα αποτελέσματά σας, αυτό θα ήταν μεγάλη. Ειδικά αν έχετε πρόσβαση σε GCP ή παρόμοια και δείτε πόσο πιο αργά ERDDAP™ πρόσβαση χρησιμοποιεί FUSE (Βασικά μπορείτε να το δοκιμάσετε και στο AWS.) . Εάν η ποινή ταχύτητας δεν είναι πολύ μεγάλη, αυτό θα ήταν υπέροχο, γιατί έχω λόγους να πιστεύω ότι κάποιοι άνθρωποι θα πρέπει σύντομα να εκτελέσουν τους ERDDAP™ s σε GCP με κατάστημα αντικειμένων. Οπότε αυτό δεν είναι θέμα θεωρητικού ενδιαφέροντος.
