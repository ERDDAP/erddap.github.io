"use strict";(globalThis.webpackChunkdocumentation=globalThis.webpackChunkdocumentation||[]).push([[5778],{28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>i});var t=s(96540);const r={},o=t.createContext(r);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},61147:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"server-admin/admin-tips/rclone-s3","title":"rclone-s3","description":"This content is based on a message from Roy Mendelssohn to the ERDDAP users group.","source":"@site/docs/server-admin/admin-tips/rclone-s3.md","sourceDirName":"server-admin/admin-tips","slug":"/server-admin/admin-tips/rclone-s3","permalink":"/ru/docs/server-admin/admin-tips/rclone-s3","draft":false,"unlisted":false,"editUrl":"https://github.com/erddap/erddap.github.io/tree/main/docs/server-admin/admin-tips/rclone-s3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docSidebar","previous":{"title":"optimizing_netcdf","permalink":"/ru/docs/server-admin/admin-tips/optimizing_netcdf"},"next":{"title":"swap-space","permalink":"/ru/docs/server-admin/admin-tips/swap-space"}}');var r=s(74848),o=s(28453);const a={},i=void 0,c={},l=[];function d(e){const n={a:"a",br:"br",code:"code",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["This content is based on a ",(0,r.jsx)(n.a,{href:"https://groups.google.com/g/erddap/c/zZUt6PKfkoI/m/expZ3UkkBAAJ",children:"message from Roy Mendelssohn to the ERDDAP users group"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"Recently, we have been getting a number of inquiries seeking help with accessing files on AWS S3 in ERDDAP\u2122. First, ERDDAP\u2122 version 2.29 will have improved S3 access which should work with non-AWS object stores also. (Thanks Seth!). But I have mentioned previously about using a FUSE based system to make the S3 store appear like a filesystem on your server or VM."}),"\n",(0,r.jsxs)(n.p,{children:["One way to do this is use \u201crclone\u201d. (",(0,r.jsx)(n.a,{href:"https://rclone.org/",children:"https://rclone.org/"}),"). rclone works on many different S3 systems, and has a lot of different settings to optimize performance, including setting a cache size, which hopefully can offset some of the speed penalty from running FUSE. The advantage of using rclone with ERDDAP \u2122 is that rclone handles all the interaction with S3, so dataset types like EDDGridFromNcFiles can be used directly as if there are local files. This means that you only need to figure out how to setup rclone to access your object store, and the rest is just normal Linux type setups."]}),"\n",(0,r.jsx)(n.p,{children:"Now I would be remiss if I just left it at that, and not give an example. In the following I am going to anonymously mount the NOAA Goes17 data that is on a public accessible AWS S3 store on one of our Ubuntu servers, In the initial setup the rclone process will be running in the foreground to make it easier to test that everything is working, and then I will discuss how to turn ii into a service running in the background. Note that in what is below, the cache is set to 1GB. Performance may well be enhanced by making the cache much larger, say 5GB-10GB or even bigger. Also the settings are my guesses at what may optimize performance, but may not be the optimal ones for ERDDAP\u2122."}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Install the necessary software:\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"sudo apt update\nsudo apt install rclone fuse3 -y"}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"Create an anonymous S3 remote\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["rclone config create goes17 s3 ",(0,r.jsx)(n.br,{}),"\n","provider AWS ",(0,r.jsx)(n.br,{}),"\n","region us-east-1 ",(0,r.jsx)(n.br,{}),"\n","location_constraint us-east-1 ",(0,r.jsx)(n.br,{}),"\n","env_auth false ",(0,r.jsx)(n.br,{}),"\n","anonymous true"]}),"\n",(0,r.jsxs)(n.ol,{start:"3",children:["\n",(0,r.jsx)(n.li,{children:"Test that.\n\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["rclone lsd goes17",":noaa-goes17"," | head"]}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsx)(n.li,{children:"Create a mount point for the data\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"sudo mkdir -p /mnt/goes17\nsudo chown $USER:$USER /mnt/goes17"}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsx)(n.li,{children:"Mount the data. (Note this process runs in the foreground, so it will show some output and sit there)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["rclone -vv mount goes17",":noaa-goes17"," /mnt/goes17 ",(0,r.jsx)(n.br,{}),"\n","--read-only ",(0,r.jsx)(n.br,{}),"\n","--vfs-cache-mode full ",(0,r.jsx)(n.br,{}),"\n","--vfs-cache-max-size 1G ",(0,r.jsx)(n.br,{}),"\n","--vfs-cache-poll-interval 1m ",(0,r.jsx)(n.br,{}),"\n","--vfs-read-chunk-size 64M ",(0,r.jsx)(n.br,{}),"\n","--vfs-read-chunk-size-limit 1G ",(0,r.jsx)(n.br,{}),"\n","--vfs-read-ahead 256M ",(0,r.jsx)(n.br,{}),"\n","--buffer-size 64M ",(0,r.jsx)(n.br,{}),"\n","--dir-cache-time 24h ",(0,r.jsx)(n.br,{}),"\n","--attr-timeout 1s ",(0,r.jsx)(n.br,{}),"\n","--no-modtime"]}),"\n",(0,r.jsxs)(n.ol,{start:"6",children:["\n",(0,r.jsx)(n.li,{children:"Open a new tab on the server and check\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"ls /mnt/goes17 | head"}),"\n",(0,r.jsxs)(n.ol,{start:"7",children:["\n",(0,r.jsx)(n.li,{children:"Check that data can be accessed\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\ncd /mnt/goes17/ABI-L1b-RadC/2023/010/15\nncdump -h OR_ABI-L1b-RadC-M6C16_G17_s20230101536138_e20230101536138_c20230101541461.nc"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'netcdf OR_ABI-L1b-RadC-M6C16_G17_s20230101536138_e20230101536138_c20230101541461 {\ndimensions:\ny = 1500 ;\nx = 2500 ;\nnumber_of_time_bounds = 2 ;\nband = 1 ;\nnumber_of_image_bounds = 2 ;\nnum_star_looks = 24 ;\nvariables:\nshort Rad(y, x) ;\nRad:_FillValue = 1023s ;\nRad:long_name = "ABI L1b Radiances" ;\nRad:standard_name = "toa_outgoing_radiance_per_unit_wavenumber" ;\nRad:_Unsigned = "true" ;\nRad:sensor_band_bit_depth = 10b ;\nRad:valid_range = 0s, 1022s ;\nRad:scale_factor = 0.1760585f ;\nRad:add_offset = -5.2392f ;\nRad:units = "mW m-2 sr-1 (cm-1)-1" ;\nRad:resolution = "y: 0.000056 rad x: 0.000056 rad" ;\nRad:coordinates = "band_id band_wavelength t y x" ;\nRad:grid_mapping = "goes_imager_projection" ;\nRad:cell_methods = "t: point area: point" ;\nRad:ancillary_variables = "DQF" ;\n.\n.\n.\n.\n'})}),"\n",(0,r.jsx)(n.p,{children:"The result was returned surprisingly quickly, particularly since our installation does not have the fastest pipe in the world."}),"\n",(0,r.jsxs)(n.ol,{start:"8",children:["\n",(0,r.jsx)(n.li,{children:"Make into a system service(modify as appropriate for user etc):\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"a. Create a systemd unit:"}),"\n",(0,r.jsx)(n.p,{children:"sudo nano /etc/systemd/system/rclone-goes17.service"}),"\n",(0,r.jsx)(n.p,{children:"And enter:"}),"\n",(0,r.jsx)(n.p,{children:"[Unit]\nDescription=Rclone mount for GOES17 public S3\nAfter=network-online.target"}),"\n",(0,r.jsxs)(n.p,{children:["[Service]\nType=simple\nUser=ubuntu\nExecStart=/usr/bin/rclone mount goes17",":noaa-goes17"," /mnt/goes17 ",(0,r.jsx)(n.br,{}),"\n","--read-only ",(0,r.jsx)(n.br,{}),"\n","--vfs-cache-mode full ",(0,r.jsx)(n.br,{}),"\n","--vfs-cache-max-size 1G ",(0,r.jsx)(n.br,{}),"\n","--vfs-cache-poll-interval 1m ",(0,r.jsx)(n.br,{}),"\n","--vfs-read-chunk-size 64M ",(0,r.jsx)(n.br,{}),"\n","--vfs-read-chunk-size-limit 1G ",(0,r.jsx)(n.br,{}),"\n","--vfs-read-ahead 256M ",(0,r.jsx)(n.br,{}),"\n","--buffer-size 64M ",(0,r.jsx)(n.br,{}),"\n","--dir-cache-time 24h ",(0,r.jsx)(n.br,{}),"\n","--attr-timeout 1s ",(0,r.jsx)(n.br,{}),"\n","--no-modtime ",(0,r.jsx)(n.br,{}),"\n","--s3-no-check-bucket\nExecStop=/bin/fusermount3 -u /mnt/goes17\nRestart=always\nRestartSec=10"]}),"\n",(0,r.jsx)(n.p,{children:"[Install]\nWantedBy=multi-user.target"}),"\n",(0,r.jsx)(n.p,{children:"b. Enable the service and start:"}),"\n",(0,r.jsx)(n.p,{children:"sudo systemctl daemon-reload\nsudo systemctl enable --now rclone-goes17"}),"\n",(0,r.jsx)(n.p,{children:"c. Test"}),"\n",(0,r.jsx)(n.p,{children:"systemctl status rclone-goes17\nls /mnt/goes17 | head"}),"\n",(0,r.jsx)(n.p,{children:"Hopefully this will be of use to people. We have been testing using gcsfuse on Google Cloud Platform with a bucket that has hierarchical name space with some success. One advantage of rclone (besides that it is not vendor specific) is that it has more settings to optimize performance. Particularly if you are moving a local ERDDAP\u2122 to the cloud, this can make the transition almost seamless."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);